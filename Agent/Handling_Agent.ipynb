{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9117b888",
   "metadata": {},
   "source": [
    "# 1 - Importa bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab0f0694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from torch.nn import Tanh, ELU, ReLU, Sigmoid, Softmax\n",
    "import websocket\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import random\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import stable_baselines3\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Tuple, MultiDiscrete\n",
    "from stable_baselines3 import PPO, DQN, A2C, SAC, TD3, DDPG\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList, ProgressBarCallback, TensorboardCallback, EvalCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from sb3_contrib import RecurrentPPO, TQC, QRDQN, MaskablePPO, TRPO, ARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca9b02a",
   "metadata": {},
   "source": [
    "# 2 - Cria funções de encoding/decoding em json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76215066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):            \n",
    "            return obj.tolist()\n",
    "        \n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "identifier ='Handling_manual'\n",
    "actions =[]\n",
    "message = ''\n",
    "\n",
    "\n",
    "def encode_json(identifier, actions):\n",
    "    data = {}\n",
    "    data['identifier'] = identifier\n",
    "    data['actions'] = actions\n",
    "    json_data = json.dumps(data, cls=NumpyEncoder)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9599a2",
   "metadata": {},
   "source": [
    "# 3 - Define função de criação do agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d38498f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrillEnv(Env):\n",
    "    \"\"\"Custom Environment that follows gym interface\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def _reset(self):\n",
    "        \n",
    "        self.reward = 0\n",
    "        self.action_name = ''\n",
    "        self.ConveyorFinish = False\n",
    "        self.GripperClose = False\n",
    "        self.Picked = False\n",
    "        self.GantryYposition = 0\n",
    "        self.GantryYatDestination = False\n",
    "        self.GantryZPosition = 0\n",
    "        self.GantryZatDestination = False\n",
    "        self.GantryYdriving = False\n",
    "        self.GantryZdriving = False\n",
    "        self.BoxSensor = False\n",
    "        self.ChangeCycle = False\n",
    "        actions = []\n",
    "        self.number_steps = 0\n",
    "        self.rewards = []\n",
    "        self.buffer_size = tamanho_buffer\n",
    "        self.state_buffer = np.zeros((self.buffer_size * 6))\n",
    "        self.rewards.append(0) \n",
    "        \n",
    "    def _obs(self):\n",
    "        obs= self.state_buffer                     \n",
    "        return obs\n",
    "        \n",
    "\n",
    "    def setprint(self, print):\n",
    "        self.print = print \n",
    "    def setprint2(self, print):\n",
    "        self.print2 = print\n",
    "    \n",
    "    def __init__(self):     \n",
    "        super(DrillEnv, self).__init__()\n",
    "        self.reset()\n",
    "\n",
    "        self.print = False\n",
    "        self.print2 = False\n",
    "        self.action_space = Discrete(11)\n",
    "        self.observation_space = Box(low=0, high=1, shape=(1,self.buffer_size * 6), dtype=np.float32)\n",
    "        \n",
    "    def buffer(self, ConveyorFinish, Picked, GantryYposition, \n",
    "                   GantryZPosition, GripperClose, ChangeCycle):\n",
    "\n",
    "        self.state_buffer = np.roll(self.state_buffer,6)         \n",
    "        self.state_buffer[0] = ConveyorFinish \n",
    "        self.state_buffer[1] = Picked \n",
    "        self.state_buffer[2] = GantryYposition\n",
    "        self.state_buffer[3] = GantryZPosition \n",
    "        self.state_buffer[4] = GripperClose \n",
    "        self.state_buffer[5] = ChangeCycle\n",
    "        \n",
    "    def step(self, action):\n",
    "        actions = action\n",
    "        \n",
    "        self.number_steps += 1                 \n",
    "        mensagem = encode_json('Handling_manual',  [actions])        \n",
    "        ws.send(mensagem)\n",
    "\n",
    "        data =''\n",
    "        station_identifier = ''\n",
    "        while  station_identifier != 'Handling':\n",
    "            try:\n",
    "                data = json.loads(ws.recv())                \n",
    "                station_identifier = (data['identifier'])\n",
    "            except:\n",
    "                return\n",
    "        \n",
    "        self.reward = int(data['reward'])\n",
    "         \n",
    "        if bool(data['done']) == False:\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "               \n",
    "        self.ConveyorFinish = bool(data['states'][0])\n",
    "        self.Picked = bool(data['states'][1])\n",
    "        self.GantryYposition = float(data['states'][2])\n",
    "        self.GantryZPosition = float(data['states'][3])\n",
    "        self.GripperClose = bool(data['states'][4])\n",
    "        self.ChangeCycle = bool(data['states'][5])\n",
    "                \n",
    "        self.buffer(self.ConveyorFinish, self.Picked, self.GantryYposition,self.GantryZPosition, self.GripperClose, self.ChangeCycle) \n",
    "        info = {}\n",
    "        obs=self._obs()\n",
    "        \n",
    "        \n",
    "        actions_dict = {\n",
    "                            0: {'gripClose': False},\n",
    "                            1: {'gripClose': True},\n",
    "                            2: {'GantryZposition': 0, 'GantryYposition': 0},\n",
    "                            3: {'GantryZposition': 200, 'GantryYposition': 0},\n",
    "                            4: {'GantryZposition': 0, 'GantryYposition': 430},\n",
    "                            5: {'GantryZposition': 200, 'GantryYposition': 430},\n",
    "                            6: {'GantryZposition': 0, 'GantryYposition': 530},\n",
    "                            7: {'GantryZposition': 200, 'GantryYposition': 530},\n",
    "                            8: {'GantryZposition': 0, 'GantryYposition': 630},\n",
    "                            9: {'GantryZposition': 200, 'GantryYposition': 630},\n",
    "                            10: {'rowFinished': True}\n",
    "                        }\n",
    "        \n",
    "        if self.print:  \n",
    "            print((self.number_steps),'recompensa: ',(self.reward),'|  acao --> ', actions_dict[actions],'  done: ', done)\n",
    "            for name, value in zip(['ConveyorFinish', 'Picked', 'GantryYposition', 'GantryZPosition', 'GripperClose', 'ChangeCycle'], self.state_buffer[0:6]):\n",
    "                print(f\"{name}: {value}\")\n",
    "            print('\\n')\n",
    "\n",
    "        # Return step information\n",
    "        return obs, self.reward, done, info\n",
    "    \n",
    "    def render(self , mode):\n",
    "        pass\n",
    "    def reset(self):        \n",
    "        self._reset()\n",
    "        return self._obs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198c5551",
   "metadata": {},
   "source": [
    "# 4 - Cria o modelo em Aprendizagem por Reforço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eeed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "algoritmo = TRPO\n",
    "nome_do_ficheiro = \"TRPO\"\n",
    "tamanho_buffer = 7 # tamanho do buffer aplicado ao algoritmo\n",
    "funcao_ativacao = Tanh #ex: Tanh, ELU, ReLU, Sigmoid, Softmax\n",
    "rede_neuronal=[dict(pi=[60,60], vf=[60,60])] # define número de nurónios na rede neuronal de política e rede neuronal de valor\n",
    "tipo_camadas = 'MlpPolicy' # define o tipo de camadas do modelo !!---(Para modelo em PPORecurrent, usar MlpLstmPolicy)---!!\n",
    "fator_desconto= 0.95 #fator de desconto para aprendizagem por reforço\n",
    "save_path = os.path.join('Training_Handling','Model_saves',f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c516b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training_Handling','Model_saves',f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}\"\"\")\n",
    "policy_kwargs = dict(activation_fn=funcao_ativacao,net_arch=rede_neuronal)\n",
    "log_path = os.path.join('Training_Handling','Logs',f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}\"\"\")\n",
    "\n",
    "env=DrillEnv()\n",
    "env.setprint(False) ### imprime os estados e ações\n",
    "env=DummyVecEnv([lambda: env])\n",
    "env.reset()\n",
    "\n",
    "\n",
    "model=algoritmo(tipo_camadas,env,verbose=1,\n",
    "        gamma=fator_desconto, gae_lambda=0.95,\n",
    "        seed=9,policy_kwargs=policy_kwargs, \n",
    "        tensorboard_log=log_path)\n",
    "\n",
    "\n",
    "print(model.policy)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84904025",
   "metadata": {},
   "source": [
    "# 5 - Define funções de callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e46b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=10000,\n",
    "  save_path=os.path.join('Training_Handling','Model_saves',f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}_checkpoint\"\"\"),\n",
    "  name_prefix=f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}_checkpoint\"\"\")\n",
    "\n",
    "\n",
    "eval_callback = EvalCallback(env, n_eval_episodes=3,\n",
    "                             best_model_save_path=os.path.join('Training_Handling','Model_saves',f\"\"\"{nome_do_ficheiro}_B{tamanho_buffer}_backup\"\"\",'Best_model'),\n",
    "                             log_path=log_path, eval_freq=4096,\n",
    "                             deterministic=False, render=False)\n",
    "callback = CallbackList([checkpoint_callback, ProgressBarCallback(), TensorboardCallback(), eval_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca01b8",
   "metadata": {},
   "source": [
    "# 6 - Inicia treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702db40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = websocket.WebSocket()\n",
    "\n",
    "ws.connect(\"ws://127.0.0.1:12000\")\n",
    "time.sleep(1)\n",
    "\n",
    "for i in range(1):\n",
    "    model.learn(total_timesteps=15000,log_interval=200,callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764cd425",
   "metadata": {},
   "source": [
    "# 7 - Salva o modelo (caso necessário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b4814b",
   "metadata": {},
   "source": [
    "# 8 - Carrega o modelo (caso necessário)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51187987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando ficheiro:  A2C no caminho:  Training_Handling\\Model_saves\\A2C_B3\n",
      "\n",
      " **************************************************************************************************** \n",
      "\n",
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=18, out_features=60, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=18, out_features=60, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=60, out_features=60, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=60, out_features=11, bias=True)\n",
      "  (value_net): Linear(in_features=60, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "nome_do_ficheiro_carregar = \"A2C\" ###\n",
    "algoritmo_carregar = A2C ### Algoritmo a carregar\n",
    "tamanho_buffer_carregar = 3\n",
    "tamanho_buffer = tamanho_buffer_carregar\n",
    "\n",
    "load_path = os.path.join('Training_Handling','Model_saves',f\"\"\"{nome_do_ficheiro_carregar}_B{tamanho_buffer_carregar}\"\"\")\n",
    "model = algoritmo_carregar.load(load_path)\n",
    "print(\"Carregando ficheiro: \",nome_do_ficheiro_carregar, \"no caminho: \", load_path)\n",
    "print(\"\\n\",\"*\" * 100,\"\\n\" )\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0d36e",
   "metadata": {},
   "source": [
    "# 9 - Utiliza modelo treinado no ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43a610aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 0.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 0.0\n",
      "GantryZPosition: 0.0\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 1.0\n",
      "\n",
      "\n",
      "2 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 0.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 1.0\n",
      "\n",
      "\n",
      "3 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 0.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 1.0\n",
      "\n",
      "\n",
      "4 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 0.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 1.0\n",
      "\n",
      "\n",
      "5 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n",
      "6 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n",
      "7 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n",
      "8 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n",
      "9 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n",
      "10 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n",
      "11 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n",
      "12 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n",
      "13 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n",
      "14 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n",
      "15 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n",
      "16 recompensa:  0 |  acao -->  {'GantryZposition': 200, 'GantryYposition': 630}   done:  False\n",
      "ConveyorFinish: 1.0\n",
      "Picked: 0.0\n",
      "GantryYposition: 1.0\n",
      "GantryZPosition: 0.95\n",
      "GripperClose: 0.0\n",
      "ChangeCycle: 0.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \n\u001b[0;32m     17\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     19\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender() \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done: \n",
      "File \u001b[1;32mC:\\Python\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mstep(\n\u001b[0;32m     44\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions[env_idx]\n\u001b[0;32m     45\u001b[0m         )\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "\n",
    "env=DrillEnv()\n",
    "env.setprint(True) ### Imprime estados e ações\n",
    "env=DummyVecEnv([lambda: env])\n",
    "env.reset()\n",
    "\n",
    "  \n",
    "ws = websocket.WebSocket()\n",
    "ws.connect(\"ws://127.0.0.1:12000\")\n",
    "time.sleep(1)\n",
    "mensagem = encode_json('CanConveyor_automatico', []) ### Para deixar a esteira de latas no modo automático\n",
    "ws.send(mensagem)\n",
    "mensagem = encode_json('BoxConveyor_automatico', []) ### Para deixar a esteira de caixa de latas no modo automático\n",
    "time.sleep(1)\n",
    "ws.send(mensagem)\n",
    "obs = env.reset()\n",
    "while True: \n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render() \n",
    "    if done: \n",
    "        obs = env.reset()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40e6d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
